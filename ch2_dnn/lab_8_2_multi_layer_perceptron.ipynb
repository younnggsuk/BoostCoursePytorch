{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "XOR 문제를 Multi-layer perceptron으로 해결해보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2, 2, bias=True)\n",
    "        self.linear2 = nn.Linear(2, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = SimpleMLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   100/10000 Loss: 0.693165\n",
      "Epoch   200/10000 Loss: 0.693158\n",
      "Epoch   300/10000 Loss: 0.693152\n",
      "Epoch   400/10000 Loss: 0.693146\n",
      "Epoch   500/10000 Loss: 0.693141\n",
      "Epoch   600/10000 Loss: 0.693136\n",
      "Epoch   700/10000 Loss: 0.693130\n",
      "Epoch   800/10000 Loss: 0.693122\n",
      "Epoch   900/10000 Loss: 0.693113\n",
      "Epoch  1000/10000 Loss: 0.693100\n",
      "Epoch  1100/10000 Loss: 0.693082\n",
      "Epoch  1200/10000 Loss: 0.693057\n",
      "Epoch  1300/10000 Loss: 0.693020\n",
      "Epoch  1400/10000 Loss: 0.692961\n",
      "Epoch  1500/10000 Loss: 0.692867\n",
      "Epoch  1600/10000 Loss: 0.692705\n",
      "Epoch  1700/10000 Loss: 0.692400\n",
      "Epoch  1800/10000 Loss: 0.691740\n",
      "Epoch  1900/10000 Loss: 0.689995\n",
      "Epoch  2000/10000 Loss: 0.683946\n",
      "Epoch  2100/10000 Loss: 0.656723\n",
      "Epoch  2200/10000 Loss: 0.435805\n",
      "Epoch  2300/10000 Loss: 0.136151\n",
      "Epoch  2400/10000 Loss: 0.066670\n",
      "Epoch  2500/10000 Loss: 0.042328\n",
      "Epoch  2600/10000 Loss: 0.030540\n",
      "Epoch  2700/10000 Loss: 0.023720\n",
      "Epoch  2800/10000 Loss: 0.019314\n",
      "Epoch  2900/10000 Loss: 0.016250\n",
      "Epoch  3000/10000 Loss: 0.014003\n",
      "Epoch  3100/10000 Loss: 0.012289\n",
      "Epoch  3200/10000 Loss: 0.010940\n",
      "Epoch  3300/10000 Loss: 0.009852\n",
      "Epoch  3400/10000 Loss: 0.008957\n",
      "Epoch  3500/10000 Loss: 0.008208\n",
      "Epoch  3600/10000 Loss: 0.007573\n",
      "Epoch  3700/10000 Loss: 0.007027\n",
      "Epoch  3800/10000 Loss: 0.006553\n",
      "Epoch  3900/10000 Loss: 0.006138\n",
      "Epoch  4000/10000 Loss: 0.005772\n",
      "Epoch  4100/10000 Loss: 0.005446\n",
      "Epoch  4200/10000 Loss: 0.005155\n",
      "Epoch  4300/10000 Loss: 0.004892\n",
      "Epoch  4400/10000 Loss: 0.004655\n",
      "Epoch  4500/10000 Loss: 0.004440\n",
      "Epoch  4600/10000 Loss: 0.004243\n",
      "Epoch  4700/10000 Loss: 0.004062\n",
      "Epoch  4800/10000 Loss: 0.003897\n",
      "Epoch  4900/10000 Loss: 0.003744\n",
      "Epoch  5000/10000 Loss: 0.003602\n",
      "Epoch  5100/10000 Loss: 0.003471\n",
      "Epoch  5200/10000 Loss: 0.003349\n",
      "Epoch  5300/10000 Loss: 0.003234\n",
      "Epoch  5400/10000 Loss: 0.003128\n",
      "Epoch  5500/10000 Loss: 0.003028\n",
      "Epoch  5600/10000 Loss: 0.002934\n",
      "Epoch  5700/10000 Loss: 0.002846\n",
      "Epoch  5800/10000 Loss: 0.002763\n",
      "Epoch  5900/10000 Loss: 0.002685\n",
      "Epoch  6000/10000 Loss: 0.002610\n",
      "Epoch  6100/10000 Loss: 0.002540\n",
      "Epoch  6200/10000 Loss: 0.002474\n",
      "Epoch  6300/10000 Loss: 0.002410\n",
      "Epoch  6400/10000 Loss: 0.002350\n",
      "Epoch  6500/10000 Loss: 0.002293\n",
      "Epoch  6600/10000 Loss: 0.002239\n",
      "Epoch  6700/10000 Loss: 0.002187\n",
      "Epoch  6800/10000 Loss: 0.002137\n",
      "Epoch  6900/10000 Loss: 0.002089\n",
      "Epoch  7000/10000 Loss: 0.002044\n",
      "Epoch  7100/10000 Loss: 0.002001\n",
      "Epoch  7200/10000 Loss: 0.001959\n",
      "Epoch  7300/10000 Loss: 0.001919\n",
      "Epoch  7400/10000 Loss: 0.001880\n",
      "Epoch  7500/10000 Loss: 0.001843\n",
      "Epoch  7600/10000 Loss: 0.001808\n",
      "Epoch  7700/10000 Loss: 0.001774\n",
      "Epoch  7800/10000 Loss: 0.001741\n",
      "Epoch  7900/10000 Loss: 0.001709\n",
      "Epoch  8000/10000 Loss: 0.001678\n",
      "Epoch  8100/10000 Loss: 0.001649\n",
      "Epoch  8200/10000 Loss: 0.001620\n",
      "Epoch  8300/10000 Loss: 0.001593\n",
      "Epoch  8400/10000 Loss: 0.001566\n",
      "Epoch  8500/10000 Loss: 0.001540\n",
      "Epoch  8600/10000 Loss: 0.001515\n",
      "Epoch  8700/10000 Loss: 0.001491\n",
      "Epoch  8800/10000 Loss: 0.001468\n",
      "Epoch  8900/10000 Loss: 0.001445\n",
      "Epoch  9000/10000 Loss: 0.001423\n",
      "Epoch  9100/10000 Loss: 0.001402\n",
      "Epoch  9200/10000 Loss: 0.001381\n",
      "Epoch  9300/10000 Loss: 0.001361\n",
      "Epoch  9400/10000 Loss: 0.001341\n",
      "Epoch  9500/10000 Loss: 0.001322\n",
      "Epoch  9600/10000 Loss: 0.001304\n",
      "Epoch  9700/10000 Loss: 0.001286\n",
      "Epoch  9800/10000 Loss: 0.001268\n",
      "Epoch  9900/10000 Loss: 0.001251\n",
      "Epoch 10000/10000 Loss: 0.001235\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    h = model(X)\n",
    "    \n",
    "    loss = criterion(h, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:5d}/{} Loss: {:.6f}\".format(\n",
    "            epoch, n_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Answer\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    \n",
    "    print(\"Predicted\\n\", predicted.detach().cpu().numpy())\n",
    "    print(\"Answer\\n\", Y.detach().cpu().numpy())\n",
    "    print(\"Accuracy: \", accuracy.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide & Deep MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2, 10, bias=True)\n",
    "        self.linear2 = nn.Linear(10, 10, bias=True)\n",
    "        self.linear3 = nn.Linear(10, 10, bias=True)\n",
    "        self.linear4 = nn.Linear(10, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        x = self.sigmoid(self.linear4(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = WideAndDeepMLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   100/10000 Loss: 0.693111\n",
      "Epoch   200/10000 Loss: 0.693102\n",
      "Epoch   300/10000 Loss: 0.693092\n",
      "Epoch   400/10000 Loss: 0.693081\n",
      "Epoch   500/10000 Loss: 0.693068\n",
      "Epoch   600/10000 Loss: 0.693052\n",
      "Epoch   700/10000 Loss: 0.693033\n",
      "Epoch   800/10000 Loss: 0.693010\n",
      "Epoch   900/10000 Loss: 0.692981\n",
      "Epoch  1000/10000 Loss: 0.692944\n",
      "Epoch  1100/10000 Loss: 0.692896\n",
      "Epoch  1200/10000 Loss: 0.692832\n",
      "Epoch  1300/10000 Loss: 0.692743\n",
      "Epoch  1400/10000 Loss: 0.692614\n",
      "Epoch  1500/10000 Loss: 0.692418\n",
      "Epoch  1600/10000 Loss: 0.692102\n",
      "Epoch  1700/10000 Loss: 0.691546\n",
      "Epoch  1800/10000 Loss: 0.690449\n",
      "Epoch  1900/10000 Loss: 0.687879\n",
      "Epoch  2000/10000 Loss: 0.679860\n",
      "Epoch  2100/10000 Loss: 0.641612\n",
      "Epoch  2200/10000 Loss: 0.549555\n",
      "Epoch  2300/10000 Loss: 0.508556\n",
      "Epoch  2400/10000 Loss: 0.487795\n",
      "Epoch  2500/10000 Loss: 0.390165\n",
      "Epoch  2600/10000 Loss: 0.020705\n",
      "Epoch  2700/10000 Loss: 0.007833\n",
      "Epoch  2800/10000 Loss: 0.004558\n",
      "Epoch  2900/10000 Loss: 0.003135\n",
      "Epoch  3000/10000 Loss: 0.002357\n",
      "Epoch  3100/10000 Loss: 0.001873\n",
      "Epoch  3200/10000 Loss: 0.001545\n",
      "Epoch  3300/10000 Loss: 0.001309\n",
      "Epoch  3400/10000 Loss: 0.001133\n",
      "Epoch  3500/10000 Loss: 0.000996\n",
      "Epoch  3600/10000 Loss: 0.000887\n",
      "Epoch  3700/10000 Loss: 0.000799\n",
      "Epoch  3800/10000 Loss: 0.000725\n",
      "Epoch  3900/10000 Loss: 0.000663\n",
      "Epoch  4000/10000 Loss: 0.000611\n",
      "Epoch  4100/10000 Loss: 0.000566\n",
      "Epoch  4200/10000 Loss: 0.000526\n",
      "Epoch  4300/10000 Loss: 0.000492\n",
      "Epoch  4400/10000 Loss: 0.000461\n",
      "Epoch  4500/10000 Loss: 0.000434\n",
      "Epoch  4600/10000 Loss: 0.000410\n",
      "Epoch  4700/10000 Loss: 0.000388\n",
      "Epoch  4800/10000 Loss: 0.000368\n",
      "Epoch  4900/10000 Loss: 0.000350\n",
      "Epoch  5000/10000 Loss: 0.000334\n",
      "Epoch  5100/10000 Loss: 0.000319\n",
      "Epoch  5200/10000 Loss: 0.000305\n",
      "Epoch  5300/10000 Loss: 0.000293\n",
      "Epoch  5400/10000 Loss: 0.000281\n",
      "Epoch  5500/10000 Loss: 0.000270\n",
      "Epoch  5600/10000 Loss: 0.000260\n",
      "Epoch  5700/10000 Loss: 0.000251\n",
      "Epoch  5800/10000 Loss: 0.000242\n",
      "Epoch  5900/10000 Loss: 0.000234\n",
      "Epoch  6000/10000 Loss: 0.000226\n",
      "Epoch  6100/10000 Loss: 0.000219\n",
      "Epoch  6200/10000 Loss: 0.000212\n",
      "Epoch  6300/10000 Loss: 0.000206\n",
      "Epoch  6400/10000 Loss: 0.000200\n",
      "Epoch  6500/10000 Loss: 0.000194\n",
      "Epoch  6600/10000 Loss: 0.000188\n",
      "Epoch  6700/10000 Loss: 0.000183\n",
      "Epoch  6800/10000 Loss: 0.000178\n",
      "Epoch  6900/10000 Loss: 0.000174\n",
      "Epoch  7000/10000 Loss: 0.000169\n",
      "Epoch  7100/10000 Loss: 0.000165\n",
      "Epoch  7200/10000 Loss: 0.000161\n",
      "Epoch  7300/10000 Loss: 0.000157\n",
      "Epoch  7400/10000 Loss: 0.000154\n",
      "Epoch  7500/10000 Loss: 0.000150\n",
      "Epoch  7600/10000 Loss: 0.000147\n",
      "Epoch  7700/10000 Loss: 0.000144\n",
      "Epoch  7800/10000 Loss: 0.000140\n",
      "Epoch  7900/10000 Loss: 0.000138\n",
      "Epoch  8000/10000 Loss: 0.000135\n",
      "Epoch  8100/10000 Loss: 0.000132\n",
      "Epoch  8200/10000 Loss: 0.000129\n",
      "Epoch  8300/10000 Loss: 0.000127\n",
      "Epoch  8400/10000 Loss: 0.000124\n",
      "Epoch  8500/10000 Loss: 0.000122\n",
      "Epoch  8600/10000 Loss: 0.000120\n",
      "Epoch  8700/10000 Loss: 0.000118\n",
      "Epoch  8800/10000 Loss: 0.000115\n",
      "Epoch  8900/10000 Loss: 0.000113\n",
      "Epoch  9000/10000 Loss: 0.000111\n",
      "Epoch  9100/10000 Loss: 0.000110\n",
      "Epoch  9200/10000 Loss: 0.000108\n",
      "Epoch  9300/10000 Loss: 0.000106\n",
      "Epoch  9400/10000 Loss: 0.000104\n",
      "Epoch  9500/10000 Loss: 0.000102\n",
      "Epoch  9600/10000 Loss: 0.000101\n",
      "Epoch  9700/10000 Loss: 0.000099\n",
      "Epoch  9800/10000 Loss: 0.000098\n",
      "Epoch  9900/10000 Loss: 0.000096\n",
      "Epoch 10000/10000 Loss: 0.000095\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    h = model(X)\n",
    "    \n",
    "    loss = criterion(h, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:5d}/{} Loss: {:.6f}\".format(\n",
    "            epoch, n_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Answer\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    \n",
    "    print(\"Predicted\\n\", predicted.detach().cpu().numpy())\n",
    "    print(\"Answer\\n\", Y.detach().cpu().numpy())\n",
    "    print(\"Accuracy: \", accuracy.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
