{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "퍼셉트론을 구현하고 XOR 문제를 해결하지 못하는 것을 확인해보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.sigmoid(self.linear(X))\n",
    "    \n",
    "    \n",
    "model = Perceptron().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   100/10000 Loss: 0.693148\n",
      "Epoch   200/10000 Loss: 0.693147\n",
      "Epoch   300/10000 Loss: 0.693147\n",
      "Epoch   400/10000 Loss: 0.693147\n",
      "Epoch   500/10000 Loss: 0.693147\n",
      "Epoch   600/10000 Loss: 0.693147\n",
      "Epoch   700/10000 Loss: 0.693147\n",
      "Epoch   800/10000 Loss: 0.693147\n",
      "Epoch   900/10000 Loss: 0.693147\n",
      "Epoch  1000/10000 Loss: 0.693147\n",
      "Epoch  1100/10000 Loss: 0.693147\n",
      "Epoch  1200/10000 Loss: 0.693147\n",
      "Epoch  1300/10000 Loss: 0.693147\n",
      "Epoch  1400/10000 Loss: 0.693147\n",
      "Epoch  1500/10000 Loss: 0.693147\n",
      "Epoch  1600/10000 Loss: 0.693147\n",
      "Epoch  1700/10000 Loss: 0.693147\n",
      "Epoch  1800/10000 Loss: 0.693147\n",
      "Epoch  1900/10000 Loss: 0.693147\n",
      "Epoch  2000/10000 Loss: 0.693147\n",
      "Epoch  2100/10000 Loss: 0.693147\n",
      "Epoch  2200/10000 Loss: 0.693147\n",
      "Epoch  2300/10000 Loss: 0.693147\n",
      "Epoch  2400/10000 Loss: 0.693147\n",
      "Epoch  2500/10000 Loss: 0.693147\n",
      "Epoch  2600/10000 Loss: 0.693147\n",
      "Epoch  2700/10000 Loss: 0.693147\n",
      "Epoch  2800/10000 Loss: 0.693147\n",
      "Epoch  2900/10000 Loss: 0.693147\n",
      "Epoch  3000/10000 Loss: 0.693147\n",
      "Epoch  3100/10000 Loss: 0.693147\n",
      "Epoch  3200/10000 Loss: 0.693147\n",
      "Epoch  3300/10000 Loss: 0.693147\n",
      "Epoch  3400/10000 Loss: 0.693147\n",
      "Epoch  3500/10000 Loss: 0.693147\n",
      "Epoch  3600/10000 Loss: 0.693147\n",
      "Epoch  3700/10000 Loss: 0.693147\n",
      "Epoch  3800/10000 Loss: 0.693147\n",
      "Epoch  3900/10000 Loss: 0.693147\n",
      "Epoch  4000/10000 Loss: 0.693147\n",
      "Epoch  4100/10000 Loss: 0.693147\n",
      "Epoch  4200/10000 Loss: 0.693147\n",
      "Epoch  4300/10000 Loss: 0.693147\n",
      "Epoch  4400/10000 Loss: 0.693147\n",
      "Epoch  4500/10000 Loss: 0.693147\n",
      "Epoch  4600/10000 Loss: 0.693147\n",
      "Epoch  4700/10000 Loss: 0.693147\n",
      "Epoch  4800/10000 Loss: 0.693147\n",
      "Epoch  4900/10000 Loss: 0.693147\n",
      "Epoch  5000/10000 Loss: 0.693147\n",
      "Epoch  5100/10000 Loss: 0.693147\n",
      "Epoch  5200/10000 Loss: 0.693147\n",
      "Epoch  5300/10000 Loss: 0.693147\n",
      "Epoch  5400/10000 Loss: 0.693147\n",
      "Epoch  5500/10000 Loss: 0.693147\n",
      "Epoch  5600/10000 Loss: 0.693147\n",
      "Epoch  5700/10000 Loss: 0.693147\n",
      "Epoch  5800/10000 Loss: 0.693147\n",
      "Epoch  5900/10000 Loss: 0.693147\n",
      "Epoch  6000/10000 Loss: 0.693147\n",
      "Epoch  6100/10000 Loss: 0.693147\n",
      "Epoch  6200/10000 Loss: 0.693147\n",
      "Epoch  6300/10000 Loss: 0.693147\n",
      "Epoch  6400/10000 Loss: 0.693147\n",
      "Epoch  6500/10000 Loss: 0.693147\n",
      "Epoch  6600/10000 Loss: 0.693147\n",
      "Epoch  6700/10000 Loss: 0.693147\n",
      "Epoch  6800/10000 Loss: 0.693147\n",
      "Epoch  6900/10000 Loss: 0.693147\n",
      "Epoch  7000/10000 Loss: 0.693147\n",
      "Epoch  7100/10000 Loss: 0.693147\n",
      "Epoch  7200/10000 Loss: 0.693147\n",
      "Epoch  7300/10000 Loss: 0.693147\n",
      "Epoch  7400/10000 Loss: 0.693147\n",
      "Epoch  7500/10000 Loss: 0.693147\n",
      "Epoch  7600/10000 Loss: 0.693147\n",
      "Epoch  7700/10000 Loss: 0.693147\n",
      "Epoch  7800/10000 Loss: 0.693147\n",
      "Epoch  7900/10000 Loss: 0.693147\n",
      "Epoch  8000/10000 Loss: 0.693147\n",
      "Epoch  8100/10000 Loss: 0.693147\n",
      "Epoch  8200/10000 Loss: 0.693147\n",
      "Epoch  8300/10000 Loss: 0.693147\n",
      "Epoch  8400/10000 Loss: 0.693147\n",
      "Epoch  8500/10000 Loss: 0.693147\n",
      "Epoch  8600/10000 Loss: 0.693147\n",
      "Epoch  8700/10000 Loss: 0.693147\n",
      "Epoch  8800/10000 Loss: 0.693147\n",
      "Epoch  8900/10000 Loss: 0.693147\n",
      "Epoch  9000/10000 Loss: 0.693147\n",
      "Epoch  9100/10000 Loss: 0.693147\n",
      "Epoch  9200/10000 Loss: 0.693147\n",
      "Epoch  9300/10000 Loss: 0.693147\n",
      "Epoch  9400/10000 Loss: 0.693147\n",
      "Epoch  9500/10000 Loss: 0.693147\n",
      "Epoch  9600/10000 Loss: 0.693147\n",
      "Epoch  9700/10000 Loss: 0.693147\n",
      "Epoch  9800/10000 Loss: 0.693147\n",
      "Epoch  9900/10000 Loss: 0.693147\n",
      "Epoch 10000/10000 Loss: 0.693147\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    loss = criterion(hypothesis, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:5d}/{} Loss: {:.6f}\".format(\n",
    "            epoch, n_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Answer\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    \n",
    "    print(\"Predicted\\n\", predicted.detach().cpu().numpy())\n",
    "    print(\"Answer\\n\", Y.detach().cpu().numpy())\n",
    "    print(\"Accuracy: \", accuracy.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
